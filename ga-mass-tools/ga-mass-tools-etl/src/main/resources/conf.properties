# source file dir or path
transfer.sf.src=E:/test/zhizunzuyueche/relation
# json save dst dir
transfer.sf.dst=E:/test/zhizunzuyueche/json
# companyId
transfer.sf.company.id=72300510
# area code in file
transfer.sf.area.code=440300
# sysSource's sysType in file
transfer.sf.sys.type=143
# the parse spliter,default is '\t'
transfer.sf.spliter=\t
# MAC(0),PHONE(1),IMSI(2),IMEI(3),AUTH_CODE(4),CERTIFICATE_CODE(5),ACCOUNT(6),LAST_TIME(7),LAST_PLACE(8)
# first column is MAC, second is PHONE, value is: 0,1
# first column is MAC, second is IMSI, vlaue is: 0,2
# not insert json column with value -1
transfer.sf.indexs=1;4;6;5
# the date format string
transfer.sf.format=yyyy-MM-dd HH:mm:ss
# false: Local.US; else: Local.CHINA
transfer.sf.time.local=true
# the account type if need
transfer.sf.id.type=1030036
# account is email(true or false)
transfer.sf.account.is.email=true
# the auth type if need
transfer.sf.auth.type=1021902
# the cert type if need
transfer.sf.cert.type=1021111
# true: filter name; else not filter name.
transfer.sf.name.filter=true

###################### KaiKaData Parse Cli ######################
transfer.data.src=D:/test/kaika/test
transfer.data.dir=D:/test/kaika/json
transfer.data.spliter=\t

################### default setting ###################
# pool size
transfer.sf.pool.size=10
# sleep to deal file
transfer.sf.sleep=1000
# every counts sleep
transfer.sf.sleep.counts=60000

transfer.data.main.sleep=1000
transfer.data.main.sleep.counts=60000